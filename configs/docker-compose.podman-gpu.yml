# Podman GPU override using NVIDIA CDI (Container Device Interface)
# This file is used when Podman is detected with NVIDIA GPU support
# Usage: podman-compose -f docker-compose.yml -f configs/docker-compose.podman-gpu.yml up -d
services:
  ollama:
    # Podman uses CDI device notation for GPU access
    devices:
      - nvidia.com/gpu=all
    environment:
      # NVIDIA GPU settings
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0

      # Performance optimizations (tuned for RTX PRO 4000 16GB + 24 CPU cores)
      - OLLAMA_KEEP_ALIVE=-1                # Keep models loaded indefinitely
      - OLLAMA_NUM_PARALLEL=2               # 2 concurrent requests per model (adjust per GPU)
      - OLLAMA_MAX_LOADED_MODELS=1          # Load one model at a time for 8-16GB GPUs
      - OLLAMA_FLASH_ATTENTION=1            # Enable Flash Attention
      - OLLAMA_KV_CACHE_TYPE=q8_0           # 8-bit quantized cache (halves memory)
      - OLLAMA_MAX_QUEUE=100                # Queue up to 100 requests
      - OLLAMA_HOST=0.0.0.0:11434          # Bind to all interfaces

    # Resource limits (adjust based on available system RAM)
    deploy:
      resources:
        limits:
          memory: 16G                       # Max 16GB RAM
          cpus: '8.0'                       # Max 8 CPU cores
        reservations:
          memory: 8G                        # Reserve 8GB RAM
          cpus: '4.0'                       # Reserve 4 CPU cores

    # Shared memory for parallel processing
    shm_size: '2gb'

  open-webui:
    # OpenWebUI can also benefit from GPU acceleration
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    # Resource limits for OpenWebUI
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '4.0'
        reservations:
          memory: 2G
          cpus: '2.0'
